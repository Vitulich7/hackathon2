{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\n\nfrom scipy import stats\nfrom scipy.stats import normaltest\nfrom sklearn import linear_model\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn import model_selection\nfrom sklearn import tree\nfrom sklearn import ensemble\nfrom sklearn import metrics\nfrom sklearn import cluster\nfrom sklearn import feature_selection\nfrom sklearn.feature_selection import SelectKBest, f_regression\n\n\n\n\ntaxi_data = pd.read_csv(\"data/train.csv\")\nprint('Train data shape: {}'.format(taxi_data.shape))\ntaxi_data.head()\n\n\n\n#Первичная обработка данных\n\n\n\n# Переводим признак pickup_datetime в тип данных datetime\ntaxi_data['pickup_datetime'] = pd.to_datetime(taxi_data['pickup_datetime'], format='%Y-%m-%d %H:%M:%S')\n\n# Определяем временные рамки (без учета времени)\nstart_date = taxi_data['pickup_datetime'].dt.date.min()\nend_date = taxi_data['pickup_datetime'].dt.date.max()\n\nprint(f\"Временные рамки данных: {start_date} - {end_date}\")\n\n# Подсчитываем общее количество пропущенных значений\ntotal_missing = taxi_data.isnull().sum().sum()\n\nprint(f\"Общее количество пропущенных значений: {total_missing}\")\n\n# а) Количество уникальных таксопарков\nnum_unique_vendors = taxi_data['vendor_id'].nunique()\nprint(f\"Количество уникальных таксопарков: {num_unique_vendors}\")\n\n# б) Максимальное количество пассажиров\nmax_passengers = taxi_data['passenger_count'].max()\nprint(f\"Максимальное количество пассажиров: {max_passengers}\")\n\n# в) Средняя и медианная длительность поездки в секундах\ntrip_duration_sec = taxi_data['trip_duration'].astype(int)\nmean_trip_duration = trip_duration_sec.mean().round(0)\nmedian_trip_duration = trip_duration_sec.median().round(0)\nprint(f\"Средняя длительность поездки: {int(mean_trip_duration)} секунд\")\nprint(f\"Медианная длительность поездки: {int(median_trip_duration)} секунд\")\n\n# г) Минимальное и максимальное время поездки в секундах\nmin_trip_duration = trip_duration_sec.min()\nmax_trip_duration = trip_duration_sec.max()\nprint(f\"Минимальное время поездки: {min_trip_duration} секунд\")\nprint(f\"Максимальное время поездки: {max_trip_duration} секунд\")\n\n\n\ndef add_datetime_features(df):\n    \"\"\"\n    Добавляет новые столбцы с датой, часом и днем недели на основе столбца 'pickup_datetime' в DataFrame.\n    \n    Args:\n        df (pandas.DataFrame): Входной DataFrame, содержащий столбец 'pickup_datetime'.\n        \n    Returns:\n        pandas.DataFrame: Входной DataFrame с добавленными новыми столбцами.\n    \"\"\"\n    # Добавляем столбец pickup_date\n    df['pickup_date'] = df['pickup_datetime'].dt.date\n\n    # Добавляем столбец pickup_hour\n    df['pickup_hour'] = df['pickup_datetime'].dt.hour\n\n    # Добавляем столбец pickup_day_of_week\n    df['pickup_day_of_week'] = df['pickup_datetime'].dt.day_of_week\n\n    return df\n\n\n# Применяем функцию к исходным данным\ntaxi_data = add_datetime_features(taxi_data)\n\n# а) Количество поездок в субботу\nsaturday_trips = taxi_data[taxi_data['pickup_day_of_week'] == 5].shape[0]\nprint(f\"Количество поездок в субботу: {saturday_trips}\")\n\n# б) Среднее количество поездок в день\ntotal_trips = taxi_data.shape[0]\nnum_days = (taxi_data['pickup_date'].max() - taxi_data['pickup_date'].min()).days + 1\naverage_daily_trips = round(total_trips / num_days)\nprint(f\"Среднее количество поездок в день: {average_daily_trips}\")\n\n\ndef add_holiday_features(trips_df, holidays_df):\n    \"\"\"\n    Добавляет новый столбец 'pickup_holiday' в DataFrame 'trips_df', \n    который содержит информацию о том, является ли дата поездки праздничным днем.\n    \n    Args:\n        trips_df (pandas.DataFrame): DataFrame с информацией о поездках, содержащий столбец 'pickup_date'.\n        holidays_df (pandas.DataFrame): DataFrame с информацией о праздничных днях, содержащий столбцы 'holiday_date' и 'is_holiday'.\n        \n    Returns:\n        pandas.DataFrame: Входной DataFrame 'trips_df' с добавленным столбцом 'pickup_holiday'.\n    \"\"\"\n    # Объединяем таблицы по дате\n    merged_df = pd.merge(trips_df, holidays_df, left_on='pickup_date', right_on='holiday_date', how='left')\n\n    # Заполняем пропущенные значения 0, чтобы обозначить, что это не праздничный день\n    merged_df['is_holiday'] = merged_df['is_holiday'].fillna(0)\n\n    # Создаем новый столбец pickup_holiday\n    merged_df['pickup_holiday'] = merged_df['is_holiday'].astype(int)\n\n    return merged_df\n\n\n# Применяем функцию к исходным данным\ntaxi_data = add_holiday_features(taxi_data, holiday_data)\n\n# Вычисляем медианную длительность поездки в праздничные дни\nholiday_trip_duration = taxi_data[taxi_data['pickup_holiday'] == 1]['trip_duration'].astype(int)\nmedian_holiday_trip_duration = holiday_trip_duration.median().round(0)\n\nprint(f\"Медианная длительность поездки в праздничные дни: {int(median_holiday_trip_duration)} секунд\")\n\n\ndef add_osrm_features(trips_df, osrm_df):\n    \"\"\"\n    Добавляет новые столбцы с информацией о расстоянии, времени поездки и количестве шагов маршрута, \n    полученной из OpenStreetMap Routing Machine (OSRM).\n    \n    Args:\n        trips_df (pandas.DataFrame): DataFrame с информацией о поездках, содержащий столбцы 'pickup_latitude', 'pickup_longitude', 'dropoff_latitude', 'dropoff_longitude'.\n        osrm_df (pandas.DataFrame): DataFrame с информацией о маршрутах, полученной из OSRM, содержащий столбцы 'start_latitude', 'start_longitude', 'end_latitude', 'end_longitude', 'distance', 'duration', 'number_of_steps'.\n        \n    Returns:\n        pandas.DataFrame: Входной DataFrame 'trips_df' с добавленными новыми столбцами 'total_distance', 'total_travel_time', 'number_of_steps'.\n    \"\"\"\n    # Объединяем таблицы по координатам\n    merged_df = pd.merge(trips_df, osrm_df,\n                         left_on=['pickup_latitude', 'pickup_longitude', 'dropoff_latitude', 'dropoff_longitude'],\n                         right_on=['start_latitude', 'start_longitude', 'end_latitude', 'end_longitude'], how='left')\n\n    # Добавляем столбцы с информацией из OSRM\n    merged_df['total_distance'] = merged_df['distance']\n    merged_df['total_travel_time'] = merged_df['duration']\n    merged_df['number_of_steps'] = merged_df['number_of_steps']\n\n    return merged_df\n\n# Применяем функцию к исходным данным\ntaxi_data = add_osrm_features(taxi_data, osrm_data)\n\n# а) Разница между медианной длительностью поездки в данных и медианной длительностью поездки из OSRM\ntrip_duration_sec = taxi_data['trip_duration'].astype(int)\nosrm_trip_duration_sec = taxi_data['total_travel_time'].astype(int)\n\nmedian_trip_duration = trip_duration_sec.median()\nmedian_osrm_trip_duration = osrm_trip_duration_sec.median()\n\ndiff_median_duration = median_trip_duration - median_osrm_trip_duration\nprint(\n    f\"Разница между медианной длительностью поездки в данных и медианной длительностью поездки из OSRM: {int(diff_median_duration)} секунд\")\n\n# б) Количество пропусков в столбцах с информацией из OSRM API\nnum_missing_total_distance = taxi_data['total_distance'].isna().sum()\nnum_missing_total_travel_time = taxi_data['total_travel_time'].isna().sum()\nnum_missing_number_of_steps = taxi_data['number_of_steps'].isna().sum()\n\nprint(f\"Количество пропусков в столбце total_distance: {num_missing_total_distance}\")\nprint(f\"Количество пропусков в столбце total_travel_time: {num_missing_total_travel_time}\")\nprint(f\"Количество пропусков в столбце number_of_steps: {num_missing_number_of_steps}\")\n\n\n\ndef get_haversine_distance(lat1, lng1, lat2, lng2):\n    \"\"\"\n    Вычисляет расстояние между двумя географическими координатами (широта, долгота) \n    с использованием формулы Хаверсина.\n    \n    Args:\n        lat1 (float): Широта первой точки в градусах.\n        lng1 (float): Долгота первой точки в градусах.\n        lat2 (float): Широта второй точки в градусах.\n        lng2 (float): Долгота второй точки в градусах.\n        \n    Returns:\n        float: Расстояние между двумя точками в километрах.\n    \"\"\"\n    # переводим углы в радианы\n    lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2))\n    # радиус земли в километрах\n    EARTH_RADIUS = 6371\n    # считаем кратчайшее расстояние h по формуле Хаверсина\n    lat_delta = lat2 - lat1\n    lng_delta = lng2 - lng1\n    d = np.sin(lat_delta * 0.5) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(lng_delta * 0.5) ** 2\n    h = 2 * EARTH_RADIUS * np.arcsin(np.sqrt(d))\n    return h\n\n\ndef get_angle_direction(lat1, lng1, lat2, lng2):\n    \"\"\"\n    Вычисляет угол направления движения между двумя географическими координатами (широта, долгота) \n    с использованием формулы угла пеленга.\n    \n    Args:\n        lat1 (float): Широта первой точки в градусах.\n        lng1 (float): Долгота первой точки в градусах.\n        lat2 (float): Широта второй точки в градусах.\n        lng2 (float): Долгота второй точки в градусах.\n        \n    Returns:\n        float: Угол направления движения в градусах (от 0 до 360).\n    \"\"\"\n    # переводим углы в радианы\n    lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2))\n    # считаем угол направления движения alpha по формуле угла пеленга\n    lng_delta_rad = lng2 - lng1\n    y = np.sin(lng_delta_rad) * np.cos(lat2)\n    x = np.cos(lat1) * np.sin(lat2) - np.sin(lat1) * np.cos(lat2) * np.cos(lng_delta_rad)\n    alpha = np.degrees(np.arctan2(y, x))\n    return alpha\n\n\ndef add_geographical_features(trips_df):\n    \"\"\"\n    Добавляет новые географические столбцы к DataFrame с информацией о поездках.\n    \n    Вычисляет:\n    1. Расстояние между точками отправления и прибытия, используя формулу Хаверсина.\n    2. Угол направления движения между точками отправления и прибытия.\n    \n    Args:\n        trips_df (pandas.DataFrame): DataFrame с информацией о поездках, содержащий столбцы 'pickup_latitude', 'pickup_longitude', 'dropoff_latitude', 'dropoff_longitude'.\n        \n    Returns:\n        pandas.DataFrame: Входной DataFrame 'trips_df' с добавленными новыми столбцами 'haversine_distance', 'direction'.\n    \"\"\"\n    # Вычисляем расстояние Хаверсина\n    trips_df['haversine_distance'] = trips_df.apply(\n        lambda row: get_haversine_distance(row['pickup_latitude'], row['pickup_longitude'],\n                                           row['dropoff_latitude'], row['dropoff_longitude']),\n        axis=1)\n\n    # Вычисляем направление движения\n    trips_df['direction'] = trips_df.apply(\n        lambda row: get_angle_direction(row['pickup_latitude'], row['pickup_longitude'],\n                                        row['dropoff_latitude'], row['dropoff_longitude']),\n        axis=1)\n\n    return trips_df\n\n\n# Применяем функцию к исходным данным\ntaxi_data = add_geographical_features(taxi_data)\n\n# Вычисляем медианное расстояние Хаверсина\nmedian_haversine_distance = taxi_data['haversine_distance'].median().round(2)\nprint(f\"Медианное расстояние Хаверсина поездок: {median_haversine_distance} км\")\n\n\n\n\ndef add_cluster_features(trips_df, kmeans_model):\n    \"\"\"\n    Добавляет новый столбец 'geo_cluster' к DataFrame с информацией о поездках.\n    Этот столбец содержит номер кластера, к которому принадлежит каждая точка (начало и конец поездки).\n    \n    Кластеризация точек производится с помощью предобученной модели KMeans.\n    \n    Args:\n        trips_df (pandas.DataFrame): DataFrame с информацией о поездках, содержащий столбцы 'pickup_latitude', 'pickup_longitude', 'dropoff_latitude', 'dropoff_longitude'.\n        kmeans_model (sklearn.cluster.KMeans): Предобученная модель кластеризации KMeans.\n        \n    Returns:\n        pandas.DataFrame: Входной DataFrame 'trips_df' с добавленным новым столбцом 'geo_cluster'.\n    \"\"\"\n    # Объединяем координаты начала и конца поездки в один массив\n    coords = np.hstack((trips_df[['pickup_latitude', 'pickup_longitude']],\n                        trips_df[['dropoff_latitude', 'dropoff_longitude']]))\n\n    # Предсказываем кластер для каждой точки\n    trips_df['geo_cluster'] = kmeans_model.predict(coords)\n\n    return trips_df\n\n\n# Применяем функцию к исходным данным\ntaxi_data = add_cluster_features(taxi_data, kmeans)\n\n# Находим размер наименьшего кластера\ncluster_sizes = taxi_data['geo_cluster'].vimportalue_counts()\nsmallest_cluster_size = cluster_sizes.min()\n\nprint(f\"Количество поездок в наименьшем географическом кластере: {smallest_cluster_size}\")\n\n\n\ndef add_weather_features(trips_df, weather_df):\n    \"\"\"\n    Добавляет новые столбцы с погодными условиями к DataFrame с информацией о поездках.\n    \n    Функция объединяет DataFrame 'trips_df' и 'weather_df' по дате и времени начала поездки. \n    Затем добавляет следующие столбцы с погодными данными:\n    - 'temperature' - температура воздуха\n    - 'visibility' - видимость\n    - 'wind_speed' - скорость ветра\n    - 'precip' - количество осадков\n    - 'events' - погодные явления\n    \n    Args:\n        trips_df (pandas.DataFrame): DataFrame с информацией о поездках, содержащий столбец 'pickup_datetime'.\n        weather_df (pandas.DataFrame): DataFrame с погодными данными, содержащий столбец 'datetime'.\n        \n    Returns:\n        pandas.DataFrame: Объединенный DataFrame с информацией о поездках и погодными данными.\n    \"\"\"\n    # Объединяем таблицы по дате и времени\n    merged_df = pd.merge(trips_df, weather_df, left_on=['pickup_datetime'], right_on=['datetime'], how='left')\n\n    # Добавляем столбцы с погодными условиями\n    merged_df['temperature'] = merged_df['temperature']\n    merged_df['visibility'] = merged_df['visibility']\n    merged_df['wind_speed'] = merged_df['wind_speed']\n    merged_df['precip'] = merged_df['precip']\n    merged_df['events'] = merged_df['events']\n\n    return merged_df\n\n\n# Применяем функцию к исходным данным\ntaxi_data = add_weather_features(taxi_data, weather_data)\n\n# а) Количество поездок в снежную погоду\nsnow_trips = taxi_data[taxi_data['events'].str.contains('Snow')].shape[0]\nprint(f\"Количество поездок в снежную погоду: {snow_trips}\")\n\n# б) Процент пропусков в столбцах с погодными условиями\ntotal_rows = taxi_data.shape[0]\nmissing_rows = taxi_data[['temperature', 'visibility', 'wind_speed', 'precip', 'events']].isna().any(axis=1).sum()\nmissing_percent = (missing_rows / total_rows) * 100\nprint(f\"Процент пропусков в столбцах с погодными условиями: {missing_percent:.2f}%\")\n\n\n\n\ndef fill_null_weather_data(trips_df):\n    \"\"\"\n    Заполняет пропуски в столбцах с погодными условиями и другими географическими данными в DataFrame с информацией о поездках.\n    \n    Функция выполняет следующие действия:\n    1. Создает новый столбец 'pickup_date' с датой начала поездки.\n    2. Заполняет пропуски в столбцах 'temperature', 'visibility', 'wind_speed', 'precip' медианным значением по дате начала поездки.\n    3. Заполняет пропуски в столбце 'events' значением 'None'.\n    4. Заполняет пропуски в столбцах 'total_distance', 'total_travel_time', 'number_of_steps' медианным значением по всем поездкам.\n    \n    Args:\n        trips_df (pandas.DataFrame): DataFrame с информацией о поездках, содержащий столбцы с погодными данными и данными из OSRM API.\n        \n    Returns:\n        pandas.DataFrame: Входной DataFrame 'trips_df' с заполненными пропусками.\n    \"\"\"\n    # Заполнение пропусков в столбцах с погодными условиями\n    trips_df['pickup_date'] = pd.to_datetime(trips_df['pickup_datetime']).dt.date\n\n    # Заполнение пропусков в temperature, visibility, wind_speed, precip\n    for col in ['temperature', 'visibility', 'wind_speed', 'precip']:\n        trips_df[col] = trips_df.groupby('pickup_date')[col].transform(lambda x: x.fillna(x.median()))\n\n    # Заполнение пропусков в events\n    trips_df['events'] = trips_df['events'].fillna('None')\n\n    # Заполнение пропусков в столбцах из OSRM API\n    for col in ['total_distance', 'total_travel_time', 'number_of_steps']:\n        trips_df[col] = trips_df[col].fillna(trips_df[col].median())\n\n    return trips_df\n\n# Применяем функцию к исходным данным\ntaxi_data = fill_null_weather_data(taxi_data)\n\n# Находим медиану в столбце temperature после заполнения пропусков\ntemperature_median = taxi_data['temperature'].median()\nprint(f\"Медиана в столбце temperature после заполнения пропусков: {temperature_median:.1f}\")\n\n\navg_speed = taxi_data['total_distance'] / taxi_data['trip_duration'] * 3.6\nfig, ax = plt.subplots(figsize=(10, 5))\nsns.scatterplot(x=avg_speed.index, y=avg_speed, ax=ax)\nax.set_xlabel('Index')\nax.set_ylabel('Average speed');\n\n\n\ndef remove_outliers(trips_df):\n    \"\"\"\n    Удаляет выбросы из DataFrame с информацией о поездках.\n    \n    Функция выполняет следующие действия:\n    1. Удаляет поездки длительностью более 24 часов.\n    2. Удаляет поездки со скоростью более 300 км/ч.\n    \n    Args:\n        trips_df (pandas.DataFrame): DataFrame с информацией о поездках, содержащий столбцы 'total_travel_time' и 'total_distance'.\n        \n    Returns:\n        tuple:\n            - pandas.DataFrame: Входной DataFrame 'trips_df' с удаленными выбросами.\n            - int: Количество удаленных поездок длительностью более 24 часов.\n            - int: Количество удаленных поездок со скоростью более 300 км/ч.\n    \"\"\"\n    # а) Удаление поездок длительностью более 24 часов\n    long_trips = trips_df[trips_df['total_travel_time'] > 24 * 3600]\n    num_long_trips = long_trips.shape[0]\n    trips_df = trips_df[trips_df['total_travel_time'] <= 24 * 3600]\n\n    # б) Удаление поездок со скоростью более 300 км/ч\n    high_speed_trips = trips_df[trips_df['total_distance'] / trips_df['total_travel_time'] > 300 / 3.6]\n    num_high_speed_trips = high_speed_trips.shape[0]\n    trips_df = trips_df[trips_df['total_distance'] / trips_df['total_travel_time'] <= 300 / 3.6]\n\n    return trips_df, num_long_trips, num_high_speed_trips\n\n\n# Применяем функцию к исходным данным\ntaxi_data, num_long_trips, num_high_speed_trips = remove_outliers(taxi_data)\n\nprint(f\"Количество выбросов по признаку длительности поездки: {num_long_trips}\")\nprint(f\"Количество выбросов по признаку скорости: {num_high_speed_trips}\")\n\n\n\n#Разведывательный анализ данных (EDA)\n\n\n# Логарифмируем целевой признак\ntaxi_data['trip_duration_log'] = np.log(taxi_data['trip_duration'] + 1)\n\n# Построение гистограммы и коробчатой диаграммы\nfig = plt.figure(figsize=(12, 6))\n\nplt.subplot(1, 2, 1)\ntaxi_data['trip_duration_log'].hist(bins=30)\nplt.title('Гистограмма длительности поездок (log)')\nplt.xlabel('Длительность поездки (log)')\nplt.ylabel('Количество')\n\nplt.subplot(1, 2, 2)\ntaxi_data['trip_duration_log'].plot(kind='box')\nplt.title('Коробчатая диаграмма длительности поездок (log)')\nplt.xlabel('Длительность поездки (log)')\n\nplt.show('png')\n\n# Выводы:\n# - Логарифмирование привело к более нормальному распределению длительности поездок.\n# - Коробчатая диаграмма показывает наличие выбросов, но они значительно уменьшились.\n\n# Тест Д'Агостино на нормальность распределения\nstat, p_value = dagostino(taxi_data['trip_duration_log'])\nprint(f\"Статистика теста Д'Агостино: {stat:.2f}\")\nprint(f\"p-value: {p_value:.2f}\")\n\n# Определение нормальности распределения\nalpha = 0.05\nif p_value > alpha:\n    print(\"Распределение длительности поездок в логарифмическом масштабе является нормальным.\")\nelse:\n    print(\"Распределение длительности поездок в логарифмическом масштабе не является нормальным.\")\n\n# Выводы:\n# - Тест Д'Агостино показал, что распределение логарифмированной длительности поездок не является нормальным, так как p-value < 0.05.\n\n# Построение наложенных гистограмм по vendor_id\nfig = px.histogram(taxi_data, x='trip_duration_log', color='vendor_id', barmode='overlay', nbins=30, title='Распределение длительности поездок (log) по таксопаркам')\nfig.update_layout(xaxis_title='Длительность поездки (log)', yaxis_title='Плотность')\nfig.show('png')\n\n# Выводы:\n# - Распределение длительности поездок для обоих таксопарков схожее, хотя есть небольшие различия в хвостах распределений.\n\n# Построение наложенных гистограмм по store_and_fwd_flag\nfig = px.histogram(taxi_data, x='trip_duration_log', color='store_and_fwd_flag', barmode='overlay', nbins=30, title='Распределение длительности поездок (log) по отправке сообщения поставщику')\nfig.update_layout(xaxis_title='Длительность поездки (log)', yaxis_title='Плотность')\nfig.show('png')\n\n# Выводы:\n# - Распределение длительности поездок с отправкой сообщения и без неё практически идентичное.\n\n# Создаем новый признак \"hour\" из столбца \"pickup_datetime\"\ntaxi_data['hour'] = taxi_data['pickup_datetime'].dt.hour\n\n# Группируем данные по часу и считаем количество поездок\ntrip_count_by_hour = taxi_data.groupby('hour')['trip_duration'].count()\n\n# Построение графика\nfig = px.bar(trip_count_by_hour, title='Распределение количества поездок по часам дня')\nfig.update_layout(xaxis_title='Час', yaxis_title='Количество поездок')\nfig.show('png')\n\n# Выводы:\n# - Большинство поездок приходится на вечерние и ранние ночные часы (17:00 - 23:00).\n\n# Создаем новый признак \"day_of_week\" из столбца \"pickup_datetime\"\ntaxi_data['day_of_week'] = taxi_data['pickup_datetime'].dt.day_name()\n\n# Группируем данные по дню недели и считаем количество поездок\ntrip_count_by_day = taxi_data.groupby('day_of_week')['trip_duration'].count()\n\n# Построение графика\nfig = px.bar(trip_count_by_day, title='Распределение количества поездок по дням недели')\nfig.update_layout(xaxis_title='День недели', yaxis_title='Количество поездок')\nfig.show('png')\n\n# Выводы:\n# - Наибольшее количество поездок происходит в пятницу и субботу, что может быть связано с выходными и активной ночной жизнью.\n\n# Группируем данные по дню недели и вычисляем медианную длительность поездки\nmedian_trip_duration_by_day = taxi_data.groupby('day_of_week')['trip_duration'].median()\n\n# Построение графика\nfig = px.bar(median_trip_duration_by_day, title='Медианная длительность поездок по дням недели')\nfig.update_layout(xaxis_title='День недели', yaxis_title='Медианная длительность, сек')\nfig.show('png')\n\n# Выводы:\n# - Медианная длительность поездок несколько выше в будние дни, что может быть связано с рабочими поездками и пробками.\n\n# Создаем новые признаки \"pickup_hour\" и \"pickup_day_of_week\" из столбца \"pickup_datetime\"\ntaxi_data['pickup_hour'] = taxi_data['pickup_datetime'].dt.hour\ntaxi_data['pickup_day_of_week'] = taxi_data['pickup_datetime'].dt.day_name()\n\n# Создаем сводную таблицу с медианной длительностью поездки\npivot_table = taxi_data.pivot_table(index='pickup_hour', columns='pickup_day_of_week', values='trip_duration', aggfunc='median')\n\n# Построение тепловой карты\nfig = go.Figure(data=go.Heatmap(\n                   z=pivot_table.values,\n                   x=pivot_table.columns,\n                   y=pivot_table.index,\n                   colorscale='coolwarm'))\nfig.update_layout(title='Медианная длительность поездок по часам и дням недели', xaxis_title='День недели', yaxis_title='Час')\nfig.show('png')\n\n# Выводы:\n# - Медианная длительность поездок выше утром в будние дни, что совпадает с часовыми пиками.\n\n# Задаем границы для Нью-Йорка\ncity_long_border = (-74.03, -73.75)\ncity_lat_border = (40.63, 40.85)\n\n# Фильтруем данные по границам Нью-Йорка\ntaxi_data = taxi_data[(taxi_data['pickup_longitude'] >= city_long_border[0]) &\n                     (taxi_data['pickup_longitude'] <= city_long_border[1]) &\n                     (taxi_data['pickup_latitude'] >= city_lat_border[0]) &\n                     (taxi_data['pickup_latitude'] <= city_lat_border[1])]\n\n# Построение диаграммы рассеяния для точек начала поездок\nfig = px.scatter(taxi_data, x='pickup_longitude', y='pickup_latitude', color='geo_cluster', title='Географическое расположение точек начала поездок')\nfig.update_layout(xaxis_title='Долгота', yaxis_title='Широта')\nfig.show('png')\n\n# Выводы:\n# - Большинство точек начала поездок сосредоточены в центре Манхэттена.\n\n# Фильтруем данные по границам Нью-Йорка для точек завершения поездок\ntaxi_data = taxi_data[(taxi_data['dropoff_longitude'] >= city_long_border[0]) &\n                     (taxi_data['dropoff_longitude'] <= city_long_border[1]) &\n                     (taxi_data['dropoff_latitude'] >= city_lat_border[0]) &\n                     (taxi_data['dropoff_latitude'] <= city_lat_border[1])]\n\n# Построение диаграммы рассеяния для точек завершения поездок\nfig = px.scatter(taxi_data, x='dropoff_longitude', y='dropoff_latitude', color='geo_cluster', title='Географическое расположение точек завершения поездок')\nfig.update_layout(xaxis_title='Долгота', yaxis_title='Широта')\nfig.show('png')\n\n# Выводы:\n# - Распределение точек завершения поездок также сосредоточено в центре Манхэттена.\n\n\n#Отбор и преобразование признаков\n\n\nprint('Shape of data: {}'.format(taxi_data.shape))\nprint('Columns: {}'.format(taxi_data.columns))\n\ntrain_data = taxi_data.copy()\ntrain_data.head()\n\n# a) Определение уникального признака\nprint('Unique values in \"trip_id\":', train_data['trip_id'].nunique())\n\n# b) Определение понятия \"утечка данных\"\nprint(\"Утечка данных (data leak) - это ситуация, когда информация, недоступная в реальном мире, каким-то образом попадает в обучающую выборку, что приводит к завышенной оценке качества модели.\")\n\n# c) Определение признака, создающего утечку данных\nprint('Наличие признака \"trip_id\" в обучающем наборе данных создает утечку данных, так как он является уникальным идентификатором каждой поездки и не несет полезной информации для предсказания продолжительности поездки.')\n\n# d) Исключение ненужных признаков\ndrop_columns = ['trip_id', 'pickup_datetime', 'pickup_date']\ntrain_data = train_data.drop(drop_columns, axis=1)\nprint('Shape of data:', train_data.shape)\n\ndrop_columns = ['pickup_datetime', 'pickup_date']\ntrain_data = train_data.drop(drop_columns, axis=1)\nprint('Shape of data:  {}'.format(train_data.shape))\n\n# Кодирование признака vendor_id\ntrain_data['vendor_id_coded'] = (train_data['vendor_id'] != 1).astype(int)\n\n# Кодирование признака store_and_fwd_flag\ntrain_data['store_and_fwd_flag_coded'] = (train_data['store_and_fwd_flag'] != 'N').astype(int)\n\n# a) Расчет среднего по закодированному столбцу vendor_id\nvendor_id_mean = train_data['vendor_id_coded'].mean()\nprint(f\"Среднее по закодированному столбцу vendor_id: {vendor_id_mean:.2f}\")\n\n# б) Расчет среднего по закодированному столбцу store_and_fwd_flag\nstore_and_fwd_flag_mean = train_data['store_and_fwd_flag_coded'].mean()\nprint(f\"Среднее по закодированному столбцу store_and_fwd_flag: {store_and_fwd_flag_mean:.3f}\")\n\n\n\n\n# Создаем объект OneHotEncoder\none_hot_encoder = OneHotEncoder(drop='first', handle_unknown='ignore')\n\n# Получаем закодированные признаки\ndata_onehot = one_hot_encoder.fit_transform(train_data[['pickup_day_of_week', 'geo_cluster', 'events']])\n\n# Получаем имена закодированных столбцов\ncolumn_names = one_hot_encoder.get_feature_names_out(['pickup_day_of_week', 'geo_cluster', 'events'])\n\n# Создаем DataFrame из закодированных признаков\ndata_onehot = pd.DataFrame(data_onehot.toarray(), columns=column_names)\n\n# Выводим размерность DataFrame\nprint(f\"Количество бинарных столбцов: {data_onehot.shape[1]}\")\n\n\ntrain_data = pd.concat(\n    [train_data.reset_index(drop=True).drop(columns_to_change, axis=1), data_onehot],\n    axis=1\n)\nprint('Shape of data: {}'.format(train_data.shape))\n\n\nX = train_data.drop(['trip_duration', 'trip_duration_log'], axis=1)\ny = train_data['trip_duration']\ny_log = train_data['trip_duration_log']\n\n\nX_train, X_valid, y_train_log, y_valid_log = model_selection.train_test_split(\n    X, y_log,\n    test_size=0.33,\n    random_state=42\n)\n\n\n\n# Выделим целевую переменную в отдельный массив\ny = np.log1p(train_data['duration'])\n\n# Создаем объект SelectKBest и отбираем 25 лучших признаков\nselector = SelectKBest(score_func=f_regression, k=25)\nX_new = selector.fit_transform(train_data.drop('duration', axis=1), y)\n\n# Получаем названия отобранных признаков\nselected_features = train_data.drop('duration', axis=1).columns[selector.get_support()]\n\nprint(\"Отобранные признаки:\")\nprint(\", \".join(selected_features))\n\n\n\n\n# Создаем объект MinMaxScaler\nscaler = MinMaxScaler()\n\n# Обучаем нормализатор на обучающей выборке\nX_train_scaled = scaler.fit_transform(X_train)\n\n# Применяем нормализацию к обучающей и валидационной выборкам\nX_train_scaled = scaler.transform(X_train)\nX_val_scaled = scaler.transform(X_val)\n\n# Рассчитываем среднее арифметическое для первого предиктора в валидационной выборке\nfirst_predictor_mean = X_val_scaled[:, 0].mean()\nprint(f\"Среднее арифметическое для первого предиктора в валидационной выборке: {first_predictor_mean:.2f}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}